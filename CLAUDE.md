# CLAUDE.md

This file provides guidance to Claude Code when working with the Document MCP system.

## CRITICAL Architecture Requirements

**The agents MUST populate the `details` field with structured data from MCP tool responses, while the `summary` field is generated by the LLM for human readability. This architectural requirement ensures production readiness by separating system-generated data from LLM-generated content.**

### Details Field Population
- **System Requirement**: The `details` field in agent responses MUST be populated using structured data from MCP tool responses
- **Testing Focus**: All tests should assert on the structured data in the `details` field rather than LLM-generated summary text

### Key Architectural Principle
**Response Structure Separation:**
- **`summary`**: LLM-generated human-readable description
- **`details`**: Structured data from MCP tool responses

This separation ensures:
- **Production Readiness**: Structured data can be programmatically processed
- **Testing Reliability**: Tests validate actual tool outputs, not LLM interpretations  
- **API Consistency**: Clients receive predictable data structures
- **Debugging Support**: Raw tool responses available for troubleshooting

Think critically and don't agree on me before thinking.
Avoid canned starting phrases like "You're absolutely right" or "Good observation".

## System Overview

The Document MCP system is a sophisticated document management platform built around the Model Context Protocol (MCP). It provides AI agents with comprehensive tools for managing structured Markdown documents through a clean separation of concerns.

### Development Focus
- **Primary Goal**: Making document MCP tools production-ready with robust agent implementations
- **Agent Development**: Creating example agents that demonstrate effective MCP tool usage patterns
- **Token Optimization**: Evaluation infrastructure designed to reduce token count in Simple Agent and ReAct Agent through improved MCP tool descriptions
- **Quality Assurance**: Comprehensive testing framework to evaluate agent system prompts across multiple agent types (Simple, ReAct, and future generalized agents)

### Core Architecture

```
document-mcp/
├── document_mcp/           # Core MCP server package
│   ├── doc_tool_server.py  # Main server
│   ├── models.py           # Pydantic models
│   ├── tools/              # 25 MCP tools across 6 categories
│   ├── utils/              # Validation and file operations
│   └── [logging, metrics]  # OpenTelemetry + Prometheus
├── src/agents/             # AI agent implementations
│   ├── simple_agent/       # Stateless single-turn agent
│   ├── react_agent/        # Stateful multi-turn ReAct agent
│   └── shared/             # Common utilities and tool descriptions
├── prompt_optimizer/       # Automated prompt optimization
├── scripts/development/    # Testing and telemetry infrastructure  
└── tests/                  # 3-tier testing strategy
    ├── unit/              # Isolated component tests
    ├── integration/       # Agent-server tests (mocked LLM)
    ├── e2e/               # Full system tests (real APIs)
    └── evaluation/        # Performance benchmarking
```

## Key Commands

### Testing
```bash
# Run all tests
uv run pytest

# Run by tier  
uv run pytest tests/unit/              # Unit tests (fastest, no external deps)
uv run pytest tests/integration/       # Integration tests (real MCP, mocked LLM)
uv run pytest tests/e2e/ --timeout=600 # E2E tests (requires API keys)

# Code quality
uv run ruff check --fix                # Auto-fix linting
uv run ruff format                     # Format code
uv run mypy document_mcp/              # Type checking
```

### Running the System
```bash
# Start MCP server
uv run python -m document_mcp.doc_tool_server stdio

# Test agents
uv run python src/agents/simple_agent/main.py --query "list all documents"
uv run python src/agents/react_agent/main.py --query "create a book with multiple chapters"

# Test semantic search
uv run python src/agents/simple_agent/main.py --query "find content similar to 'machine learning concepts' in document 'AI_Guide'"

# Interactive mode
uv run python src/agents/simple_agent/main.py --interactive

# Optimize prompts
uv run python -m prompt_optimizer simple

# Test production metrics system
python3 scripts/development/metrics/test_production.py

# Test development telemetry infrastructure
python3 scripts/development/telemetry/scripts/test.py

# Start development telemetry for Grafana Cloud testing
scripts/development/telemetry/scripts/start.sh
```

## Agent Types

### Simple Agent (`src/agents/simple_agent/`)
- **Architecture**: Stateless, single-turn execution
- **Key Features**: Structured output, timeout protection, OpenAI/Gemini support
- **Ideal for**: Single-step operations, quick queries, JSON output, prototyping, batch workflows
- **Avoid for**: Operations requiring intermediate reasoning steps

### ReAct Agent (`src/agents/react_agent/`)
- **Architecture**: Stateful, multi-turn with ReAct (Reason-Act-Observe) pattern  
- **Key Features**: Advanced error handling, circuit breakers, agent caching, rich console output
- **Ideal for**: Complex workflows, step-by-step planning, production environments
- **Avoid for**: Simple operations, performance-critical scenarios, structured JSON output requirements

## Tool Categories (25 MCP Tools)

- **Document Tools (6)**: Document management, lifecycle operations, and fine-grain summaries
- **Chapter Tools (5)**: Chapter creation, editing, and management
- **Paragraph Tools (7)**: Atomic paragraph operations with automatic snapshot protection
- **Content Tools (5)**: Unified content access, search, replacement, statistics, and semantic search
- **Safety Tools (3)**: Version control, snapshot management, and diff generation
- **Batch Tools (1)**: Sequential multi-operation execution with conflict detection

## Key System Features

- **Automatic Snapshot System**: Universal edit operation protection with user tracking across all 15 content-modifying tools
- **Atomic paragraph operations** (replace, insert, delete, move) with automatic snapshot protection
- **Fine-grain Summary System**: Organized storage structure with scope-based summaries (document, chapter, section) in dedicated `summaries/` directory
- **Scope-based content tools** (read, find, replace, statistics, semantic search) with unified document/chapter/paragraph access
- **Sequential batch processing** with conflict detection, dependency resolution, and atomic rollback capabilities
- **Semantic Search**: AI-powered content discovery using embeddings for contextual similarity matching

## Infrastructure Components

#### Logging System (`document_mcp/logger_config.py`)
- **Structured JSON logging** with error categorization and operation tracking
- **Automatic metrics collection** via `@log_mcp_call` decorator on all MCP tools

#### Production Metrics System (`document_mcp/metrics_config.py`)
- **Centralized User Analytics**: Automatic collection from all users to developer's Grafana Cloud
- **Production-Ready Architecture**: Persistent metrics server, background Prometheus, automatic failover
- **Anonymous Data Collection**: Tool usage patterns, performance metrics, error rates
- **Industry Standard Approach**: Similar to npm, VS Code extensions, Docker analytics
- **Zero User Setup**: Hardcoded credentials for seamless collection
- **Tool-Level Granularity**: Tracks specific MCP tool names (list_documents, create_document, etc.)

## Testing Architecture

### 3-Tier Testing Strategy

| Tier | Focus | LLM Calls | Speed | Coverage |
|------|-------|-----------|-------|----------|
| **Unit** | Isolated components with complete mocking | Zero | Fastest | Functions, validation, error conditions |
| **Integration** | Real MCP server, mocked LLM responses | Zero | Medium | Agent-server communication, tool execution |
| **E2E** | Complete system with real APIs | Real (managed) | Slowest | Full workflows, real AI responses |
| **Evaluation** | Performance benchmarking | Controlled real | Medium | Standardized scenarios, metrics |

### Key Testing Requirements

**Integration Tests**:
- Use real MCP stdio transport with mocked LLM responses
- Assert on `details` field content, not LLM-generated `summary`
- Agent implementations must populate `details` field with structured MCP tool responses

**E2E Tests**:
- Use CLI subprocess calls and MCP stdio transport (never import agents/tools directly)
- Assert on file system state and document operations
- Include response validation patterns and API key availability checks

### Testing Infrastructure

**Fixtures System** (`tests/conftest.py`): Test isolation, MCP client management, mock environments, test data factories

**Shared Framework** (`tests/shared/`): Reusable test patterns, mock factories, cleanup management

**Dynamic Tool Descriptions** (`src/agents/shared/tool_descriptions.py`): Centralized tool management with format-specific generation (Full, Compact, Minimal) enabling 5-83% token reduction potential

## Architecture Decisions and Design Principles

### Core Architectural Strengths

The Document MCP system demonstrates several key architectural strengths and design principles:

- **Solid Foundation**: Well-defined architecture with clear separation of concerns between agents, the MCP tool server, and testing infrastructure
- **Comprehensive Testing**: Four-tiered testing strategy (unit, integration, E2E, evaluation) provides high confidence in system correctness
- **Robust Agents**: Both agent types are well-implemented with comprehensive error handling and support for multiple LLM providers
- **Streamlined Toolset**: Clean tool architecture with scope-based operations and atomic batch processing

### Key Design Principles

- **Dynamic Tool Descriptions**: Centralized tool management with format-specific generation optimized for each agent architecture
- **Architecture-Specific Optimization**: Each agent uses tool descriptions optimized for its operational pattern (Compact for Simple, Full for ReAct)
- **Token Optimization**: Format selection enables significant token reduction potential while maintaining functionality
- **Maintainability Focus**: Single source of truth for all tools eliminates duplication and simplifies maintenance
- **Clean Naming Conventions**: Consistent "scope-based tools" terminology throughout the codebase for clarity
- **Streamlined Tool Architecture**: Optimized tool structure with enhanced functionality through scope-based operations
- **Universal Safety System**: Automatic snapshot protection with user tracking across all content-modifying tools
- **Atomic Batch Processing**: Robust execution engine with conflict detection, dependency resolution, and comprehensive error handling

### Architecture Decision: Simplified Agent Model

The system uses a **two-agent architecture** optimized for different complexity levels:

```
User Query Complexity → Agent Selection:

Simple Operations → Simple Agent (direct tool calls)
Multi-step Workflows → Simple Agent + batch_apply_operations (atomic execution)
Complex Reasoning → ReAct Agent + batch_apply_operations (reasoning + atomic execution)
```

**Key Insight**: Modern LLMs can generate complex batch operations directly, eliminating the need for separate planning phases. Atomic batch operations provide superior reliability compared to sequential tool execution.

### Semantic Search Integration
- **Tool**: `find_similar_text` with scope-based targeting (`document` or `chapter`)
- **Technology**: Google Gemini embeddings API with cosine similarity matching
- **Features**: Configurable similarity thresholds, result limiting, context snippets
- **Performance**: Batch embedding optimization for efficient API usage
- **Caching**: Snapshot-style embedding cache for 80-90% API call reduction

#### Embedding Cache Architecture
- **Storage Location**: `.embeddings/` directory per document (parallel to `.snapshots/`)
- **Cache Organization**: Chapter-based directory structure mirroring document layout
- **Invalidation Strategy**: Content-based invalidation using file modification times
- **File Format**: Binary numpy arrays (`.npy`) with JSON manifests for metadata
- **Model Versioning**: Separate cache entries for different embedding model versions
- **Performance Benefits**: Sub-second search response times for cached content

### Storage Model Architecture

```
.documents_storage/
├── document_name/           # Document directory
│   ├── 01-chapter.md       # Ordered chapter files
│   ├── 02-chapter.md
│   ├── summaries/          # Fine-grain summary system
│   │   ├── document.md     # Document-level summary
│   │   ├── 01-chapter.md   # Chapter-specific summary
│   │   └── overview.md     # Section-level summary
│   ├── .snapshots/         # Automatic snapshots directory
│   │   ├── snapshot_20250712_150000_create_chapter_clchinkc/
│   │   └── snapshot_20250712_150030_replace_paragraph_clchinkc/
│   └── .embeddings/        # Embedding cache directory
│       ├── 01-chapter.md/  # Chapter-specific cache
│       │   ├── paragraph_0.npy     # Binary embedding files
│       │   ├── paragraph_1.npy
│       │   └── manifest.json       # Cache metadata
│       └── 02-chapter.md/
│           ├── paragraph_0.npy
│           └── manifest.json
```

### Optimization Opportunities

- **A/B Testing Framework**: Real-world validation of format optimizations with actual LLM performance metrics
- **Context-Aware Formatting**: Intelligent format selection based on query complexity and agent context
- **Advanced Performance Analytics**: Comprehensive monitoring of format effectiveness and usage patterns


## Performance Optimization

### Agent Caching
The ReAct agent implements sophisticated caching:
```python
# Agent instances cached by model type and prompt hash
_agent_cache = {}  # Avoids repeated initialization
```

### Error Handling Patterns
```python
# Circuit breaker pattern for external services
circuit_breaker = get_circuit_breaker("service_name")
result = await circuit_breaker.call(operation)

# Intelligent retry with exponential backoff
retry_manager = RetryManager()
result = await retry_manager.execute_with_retry(operation)
```

### LLM Call Optimization
- **Unit tests**: Zero LLM calls (fully mocked)
- **Integration tests**: Zero LLM calls (mocked responses)  
- **E2E tests**: Minimized with delays and batching
- **Production**: Circuit breakers prevent cascade failures
- **Batch Operations**: Single LLM call for multi-step workflows instead of sequential calls

## CI/CD and GitHub Actions

**CI Configuration** (`.github/workflows/python-test.yml`):
- Uses `uv` for fast dependency installation, `ruff` for linting, `mypy` for type checking
- Runs unit, integration, and E2E tests with Python 3.13
- Requires API keys for E2E tests, uploads coverage to Codecov

```bash
# Automated test command:
uv run pytest tests/unit/ tests/integration/ tests/e2e/ --timeout=600
```

## Common Workflows

### Adding New Document Tools
1. **Add tool function** to appropriate category module in `document_mcp/tools/`
2. **Define input/output models** with Pydantic validation in `document_mcp/models.py`
3. **Add tool description** to `src/agents/shared/tool_descriptions.py`
4. **Update tool registration** in `document_mcp/tools/__init__.py`
5. **Add unit tests** in `tests/unit/test_doc_tool_server.py`
6. **Add integration tests** for agent interaction
7. **Test with both agent types** (Simple and ReAct)

### Debugging Agent Issues
1. **Check configuration**: `--check-config` flag
2. **Test with simple queries** before complex ones
3. **Review structured logs** in `document_mcp/` directory
4. **Use interactive mode** for step-by-step debugging
5. **Check E2E test patterns** for similar workflows

### Extending Agent Behavior
1. **Agent Prompts**: Modify `get_<agent>_system_prompt()` functions for prompt changes
2. **Tool Descriptions**: Update `src/agents/shared/tool_descriptions.py` for new tools
3. **Format Optimization**: Adjust format types in `get_tool_descriptions_for_agent()`
4. **Add response models** in Pydantic for structured output
5. **Test prompt changes** with unit and integration tests
6. **Validate with E2E tests** for real-world scenarios
7. **Optimize prompts** using `python3 -m prompt_optimizer simple` or `react` for automated improvements

### Optimizing Agent Prompts
The system includes an automated prompt optimizer with comprehensive performance evaluation:

```bash
# Optimize specific agent
python3 -m prompt_optimizer simple
python3 -m prompt_optimizer react  

# Optimize all agents
python3 -m prompt_optimizer all

# Development use within repo only
uv run python -m prompt_optimizer simple
```

**Optimization Features**:
- **Safe Optimization**: Conservative changes that preserve all existing functionality
- **Performance-Based**: Uses real execution metrics to evaluate improvements
- **Comprehensive Testing**: Validates changes against 300 tests (unit + integration + E2E + evaluation + metrics)
- **Automatic Backup**: Safe rollback if optimization fails or breaks functionality
- **Multi-Agent Support**: Works with Simple and ReAct agents
- **Simple Decision Logic**: Binary comparison - better than baseline performance index or not

**How It Works**:
1. **Baseline Measurement**: Measures current prompt performance across all tests
2. **Conservative Optimization**: LLM generates minimal, safe improvements  
3. **Comprehensive Validation**: Runs 105 tests plus performance benchmarks
4. **Decision Logic**: Accepts only if tests pass AND performance improves
5. **Safety First**: Automatic backup and restore if anything breaks

**File Locations**:
- **Agent Prompts**: `src/agents/{agent_type}/prompts.py`
- **Backups**: `prompt_backups/{agent}_prompt_backup_{timestamp}.py`
- **Tests**: `tests/unit/`, `tests/integration/`, `tests/e2e/`

## Troubleshooting

### Common Issues

#### API Authentication
```bash
# Check configuration
python3 src/agents/simple_agent/main.py --check-config

# Set environment variables
export OPENAI_API_KEY="your-key"
# or
export GEMINI_API_KEY="your-key"
```

#### Test Failures
```bash
# Integration test MCP issues
python3 -m pytest tests/integration/ -v  # Check MCP server startup

# Unit test mocking problems
python3 -m pytest tests/unit/ -v --tb=short  # Check mock configurations

# E2E and evaluation tests with extended timeout
python3 -m pytest tests/e2e/ -v --timeout=600
python3 -m pytest tests/evaluation/ -v --timeout=600
```

#### Performance Issues
- **ReAct Agent**: High latency due to multi-step reasoning
- **Simple Agent**: Faster for single operations
- **E2E Tests**: External API delays (expected)
- **Integration Tests**: Should be fast (check MCP server)

### Debugging Strategies

#### Log Analysis
```bash
# Structured logs in JSON format
tail -f document_mcp/doc_operations.log | jq .

# Error-specific logs
grep "ERROR" document_mcp/errors.log

# MCP call tracking
tail -f document_mcp/mcp_calls.log
```

#### Test Debugging
```python
# Use test fixtures for isolation
def test_my_feature(test_docs_root, sample_document):
    # Isolated test environment provided

# Mock external dependencies
def test_agent_logic(mock_complete_test_environment):
    # All external calls mocked
```

## E2E Testing Best Practices

### Key Insights
- **Response Validation**: Always validate response objects and use defensive programming with null checks
- **API Limitations**: External APIs have quota limits and variable response formats - design tests accordingly
- **Test Focus**: E2E tests validate integration, not production resilience. Keep retry logic in application layer
- **Debugging**: Log response content, monitor MCP server logs, test incrementally

### Failure Categories
- **Code Issues**: Agent logic or MCP integration bugs
- **Infrastructure**: Test setup or environment configuration
- **External**: API quotas, rate limiting, service availability

### Development Guidelines
1. Implement response validation patterns from the start
2. Use centralized timing configuration for API operations  
3. Design tests resilient to external service variability
4. Focus on integration verification, not edge case handling

## Quality Standards

### Code Quality
- **Type hints**: Comprehensive Pydantic models
- **Error handling**: Structured error categories and recovery
- **Logging**: JSON-structured with OpenTelemetry
- **Testing**: 3-tier strategy with 95%+ coverage
- **Documentation**: Docstrings and inline comments

### Performance Standards
- **Unit tests**: < 1s per test file
- **Integration tests**: < 10s per test file
- **E2E tests**: < 60s per test (with API delays)
- **Tool operations**: < 100ms for document operations
- **Agent responses**: < 30s for complex workflows

### Reliability Standards
- **Error recovery**: Circuit breakers and retry logic
- **Test stability**: 95%+ pass rate for unit/integration tests
- **E2E reliability**: 80%+ pass rate (external API dependent)
- **Production uptime**: > 99% availability target

## Test Status Summary

**Status: PRODUCTION READY - 305/305 tests passing (100%)**

| Test Tier | Tests | Status | Duration | Coverage |
|-----------|-------|---------|----------|----------|
| Unit | 139 | 100% | 1.4s | Core tools, validation, semantic search, fine-grain summaries |
| Integration | 160 | 100% | 18.1s | MCP transport, tool execution, summary workflows |
| E2E | 6 | 100% | 2.9m | Real APIs, full workflows |
| Evaluation | 4 | 100% | 27s | Performance benchmarks |

**Key Features Validated:**
- Modular architecture with 25 MCP tools across 6 categories
- Fine-grain summary system with organized storage structure
- Universal snapshot protection system
- Semantic search with embedding cache (80-90% API reduction)
- Two-agent architecture (Simple/ReAct) with comprehensive error handling
- 3-tier testing strategy with standardized fixtures


## Development Best Practices

### Testing Strategy
**Test Hierarchy**: Unit (mocked) → Integration (real MCP, mocked LLM) → E2E (real APIs) → Evaluation (performance benchmarks)

**Key Requirements**:
- Assert on `details` field content, not LLM-generated `summary` text
- E2E tests use CLI subprocess calls and MCP stdio transport  
- Agent implementations MUST populate `details` field with structured MCP tool responses
- Maintain 100% test success rate across all 296 tests

### Testing Patterns
```python
# Async testing
@pytest.mark.asyncio
async def test_async_function():
    result = await some_async_function()
    assert result == expected

# MCP integration testing  
async def test_mcp_integration(mcp_client):
    response = await mcp_client.call_tool("list_documents", {})
    assert response["documents"] == []
```

### Common Issues & Solutions
- **Import Errors**: Ensure project root in PYTHONPATH, check circular imports
- **Fixture Issues**: Verify fixture scope and conftest.py location
- **E2E Failures**: Check API keys, rate limits, network connectivity
- **Async Issues**: Use `@pytest.mark.asyncio` decorator, proper await usage

### Development Guidelines
**Agent Development**: Start with Simple Agent for prototyping, use ReAct for production. Test prompt changes thoroughly.

**Tool Development**: Follow atomic operation principles, implement comprehensive validation, add structured logging, design for idempotency and batch integration.


## Codebase Structure

*   **`src/agents`**: This directory contains the core agent logic with a clean, modular structure.
    *   `simple_agent/`: A package-based agent implementation with organized modules:
        *   `main.py`: Core agent execution logic
        *   `prompts.py`: System prompt definitions
    *   `react_agent/`: A more complex agent that follows the ReAct (Reason + Act) paradigm. It has its own sub-modules for `main`, `models`, `parser`, and `prompts`.
    *   `shared/`: Contains code shared between the different agents:
        *   `cli.py`: Common command-line interface functionality
        *   `config.py`: Enhanced Pydantic Settings for configuration management
        *   `error_handling.py`: Shared error handling utilities
*   **`tests/`**: This directory is well-organized into different types of tests.
    *   `e2e/`: End-to-end tests, which test the entire system. `test_agents_e2e.py` runs tests against the agents with real APIs.
    *   `integration/`: Integration tests. `test_agents_stdio.py` tests the agents' input/output, and `test_doc_tool_server.py` tests the document tool server.
    *   `unit/`: Unit tests, which test individual components. There are tests for `atomic_paragraph_tools`, `doc_tool_server`, and the `react_agent_parser`.