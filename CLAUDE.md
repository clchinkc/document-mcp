# CLAUDE.md

This file provides guidance to Claude Code when working with the Document MCP system.

## CRITICAL Architecture Requirements

**The agents MUST populate the `details` field with structured data from MCP tool responses, while the `summary` field is generated by the LLM for human readability. This architectural requirement ensures production readiness by separating system-generated data from LLM-generated content.**

### Details Field Population
- **System Requirement**: The `details` field in agent responses MUST be populated using structured data from MCP tool responses
- **Testing Focus**: All tests should assert on the structured data in the `details` field rather than LLM-generated summary text

### Key Architectural Principle
**Response Structure Separation:**
- **`summary`**: LLM-generated human-readable description
- **`details`**: Structured data from MCP tool responses

This separation ensures:
- **Production Readiness**: Structured data can be programmatically processed
- **Testing Reliability**: Tests validate actual tool outputs, not LLM interpretations  
- **API Consistency**: Clients receive predictable data structures
- **Debugging Support**: Raw tool responses available for troubleshooting

Think critically and don't agree on me before thinking.
Avoid canned starting phrases like "You're absolutely right" or "Good observation".

## System Overview

The Document MCP system is a document management platform built around the Model Context Protocol (MCP). It provides AI agents with tools for managing structured Markdown documents.

### MCP Design Patterns

For MCP design patterns, see:

**[MCP Design Patterns Guide](docs/MCP_DESIGN_PATTERNS.md)** - Essential patterns for:
- Reference-by-handle architecture for context offloading
- Token-aware hydration strategies
- Security best practices (ephemeral auth, stderr logging)
- Tool vs. Resource design principles
- Production deployment checklist
- Real-world implementation examples

**Key Patterns Implemented**:
- **Pagination System**: Page-based disclosure (50K chars ≈ 12K tokens per page)
- **Scope-Based Tools**: Unified document/chapter/paragraph access with fine-grained control
- **Resource URIs**: Read-only content access via structured URIs (e.g., `document://<name>?page=2`)
- **Safety Tools**: Version control with snapshot-based state management
- **Semantic Search**: Vector-based content discovery with embedding cache

### Development Focus
- **Agents**: Example agents demonstrating MCP tool usage patterns
- **Tool Optimization**: A/B testing framework with multi-model support (GPT-5 Mini, Claude Haiku 4.5, Gemini 3 Flash)
- **Testing**: 4-tier test strategy (unit, integration, E2E, evaluation)
- **Prompt Optimization**: DSPy integration with composite scoring (60% accuracy + 25% input tokens + 15% output tokens)

### Core Architecture

```
document-mcp/
├── document_mcp/           # Core MCP server package
│   ├── doc_tool_server.py  # Main server
│   ├── models.py           # Pydantic models with pagination support
│   ├── tools/              # 32 MCP tools across 8 categories
│   ├── utils/              # Validation and file operations
│   ├── observability.py    # GCP-native logs/traces/metrics
│   └── [logging, metrics]  # OpenTelemetry + legacy Prometheus
├── src/agents/             # AI agent implementations
│   ├── simple_agent/       # Stateless single-turn agent
│   ├── react_agent/        # Stateful multi-turn ReAct agent
│   └── shared/             # Common utilities and tool descriptions
├── benchmarks/             # Shared benchmarking infrastructure
│   ├── config.py           # Flexible benchmark configuration
│   ├── metrics.py          # Composite scoring (accuracy + tokens)
│   ├── scenarios.py        # Level 1-4 test scenarios
│   └── tools.py            # Tool set definitions (atomic/consolidated)
├── dspy_optimizer/         # DSPy prompt optimization
├── prompt_optimizer/       # Automated prompt optimization
├── scripts/development/    # Testing and telemetry infrastructure
└── tests/                  # 4-tier testing strategy
    ├── unit/              # Isolated component tests (341)
    ├── integration/       # Agent-server tests (168)
    ├── e2e/               # Full system tests (6)
    └── evaluation/        # Performance benchmarking (21)
```

## Package Structure and Agent Separation

### Core MCP Package
The **document-mcp** package contains only the core MCP server and tools:
```bash
pip install document-mcp  # Installs MCP server and 32 tools
```

**Package Contents:**
- MCP server (`document_mcp/doc_tool_server.py`)
- 32 MCP tools across 8 categories
- GCP-native observability (`document_mcp/observability.py`)
- OpenTelemetry metrics collection (legacy + modern)
- Pydantic models and validation
- Production logging and error handling

### Development Agents (Separate)
The **agents** are development examples that run from source:
```bash
git clone https://github.com/your-org/document-mcp.git
cd document-mcp
uv sync --dev  # Install agent dependencies
```

**Agent Structure:**
- `src/agents/simple_agent/` - Stateless single-turn agent
- `src/agents/react_agent/` - Multi-turn ReAct agent  
- `src/agents/shared/` - Common utilities and tool descriptions
- `AGENTS.md` - Complete usage guide for both agents

### Why This Separation?
- **Focus**: MCP package is lean
- **Development Flexibility**: Agents can evolve independently  
- **User Choice**: Users install MCP tools, optionally clone for agent examples
- **Dependency Management**: Core package has minimal dependencies, agents have full dev stack

## Key Commands

### Testing
```bash
# Run all tests
uv run pytest

# Run by tier
uv run pytest tests/unit/              # Unit tests
uv run pytest tests/integration/       # Integration tests
uv run pytest tests/e2e/               # E2E tests (requires API keys)
uv run pytest tests/evaluation/        # Evaluation tests

# Code quality
uv run ruff check --fix                # Auto-fix linting
uv run ruff format                     # Format code
uv run mypy document_mcp/              # Type checking
```

### Running the System
```bash
# Install MCP package first
pip install document-mcp

# Start MCP server
uv run python -m document_mcp.doc_tool_server stdio

# Run agents (requires repository clone and uv sync --dev)
uv run python src/agents/simple_agent/main.py --query "list all documents"
uv run python src/agents/react_agent/main.py --query "create a book with multiple chapters"

# Test semantic search
uv run python src/agents/simple_agent/main.py --query "find content similar to 'machine learning concepts' in document 'AI_Guide'"

# Interactive mode
uv run python src/agents/simple_agent/main.py --interactive

# Optimize prompts
uv run python -m prompt_optimizer simple

# Test production metrics system
python3 scripts/development/metrics/test_production.py

# Test development telemetry infrastructure
python3 scripts/development/telemetry/scripts/test.py

# Start development telemetry for Grafana Cloud testing
scripts/development/telemetry/scripts/start.sh
```

## Agent Types

### Simple Agent (`src/agents/simple_agent/`)
- **Architecture**: Stateless, single-turn execution
- **Key Features**: Structured output, timeout protection, OpenAI/Gemini support
- **Ideal for**: Single-step operations, quick queries, JSON output, prototyping
- **Avoid for**: Operations requiring intermediate reasoning steps

### ReAct Agent (`src/agents/react_agent/`)
- **Architecture**: Stateful, multi-turn with ReAct (Reason-Act-Observe) pattern  
- **Key Features**: Error handling, circuit breakers, agent caching, rich console output
- **Ideal for**: Complex workflows, step-by-step planning
- **Avoid for**: Simple operations, performance-critical scenarios, structured JSON output requirements

## Tool Categories (32 MCP Tools)

- **Document Tools (6)**: Document management, lifecycle operations, and fine-grain summaries
- **Chapter Tools (4)**: Chapter creation, editing, listing, and management with frontmatter metadata support
- **Paragraph Tools (8)**: Atomic paragraph operations (read, replace, insert, delete, move, append) with automatic snapshot protection
- **Content Tools (6)**: Unified content access with pagination, search, replacement, statistics, semantic search, and entity tracking
- **Metadata Tools (3)**: YAML-based metadata management for chapters, entities, and timeline events
- **Safety Tools (3)**: Version control, snapshot management, and diff generation
- **Overview Tools (1)**: Document outline with metadata and entity counts
- **Discovery Tools (1)**: Tool search and discovery

## Key System Features

- **Automatic Snapshot System**: Universal edit operation protection with user tracking across all 15 content-modifying tools
- **Atomic paragraph operations** (replace, insert, delete, move) with automatic snapshot protection
- **Fine-grain Summary System**: Organized storage structure with scope-based summaries (document, chapter, section) in dedicated `summaries/` directory
- **Pagination System**: Page-based content retrieval (50K chars ≈ 12K tokens per page) with metadata, navigation hints, and memory-efficient bounded loading
- **Scope-based content tools** (read, find, replace, statistics, semantic search) with unified document/chapter/paragraph access
- **Semantic Search**: AI-powered content discovery using embeddings for contextual similarity matching
- **Metadata System (Phase 2)**: YAML-based structured metadata for chapters (frontmatter), entities (characters, locations, items), and timeline events with Gemini-compatible tool parameters

## Infrastructure Components

#### Logging System (`document_mcp/logger_config.py`)
- **Structured JSON logging** with error categorization and operation tracking
- **Automatic metrics collection** via `@log_mcp_call` decorator on all MCP tools

#### Production Metrics System (`document_mcp/metrics_config.py`)
- **OpenTelemetry Integration**: Standard metrics collection with Prometheus endpoint
- **Local Prometheus**: Metrics available at `http://localhost:8001/metrics`
- **Tool-Level Granularity**: Tracks specific MCP tool names (list_documents, create_document, etc.)
- **Composite Scoring**: 60% accuracy + 25% input token efficiency + 15% output token efficiency (inverse scaling: `eff = 1/(1 + tokens/scale)`)

#### GCP Observability (`document_mcp/observability.py`)
- **Cloud Logging**: Native integration via `google-cloud-logging`
- **Cloud Trace**: Distributed tracing via `CloudTraceSpanExporter`
- **Cloud Monitoring**: Metrics export via `CloudMonitoringMetricsExporter`
- **Auto-Detection**: Automatic Cloud Run detection via `K_SERVICE` environment variable
- **Graceful Fallback**: Uses safe console exporters locally for development

**Using Observability Locally:**
```python
from document_mcp.observability import (
    initialize_observability,
    get_observability_status,
    trace_mcp_tool,
    log_mcp_call,
)

# Initialize (call once at startup)
initialize_observability()

# Check status
status = get_observability_status()
# Returns: {'enabled': True, 'environment': 'local', 'tracing': True, 'metrics': True, ...}

# Trace a tool call
with trace_mcp_tool('my_tool', param='value'):
    # tool implementation
    pass

# Or use as decorator
@log_mcp_call
def my_mcp_tool():
    pass
```

**Environment Variables:**
- `MCP_OBSERVABILITY_ENABLED`: Set to "false" to disable (default: "true")
- `K_SERVICE`: Set automatically on Cloud Run (triggers GCP exporters)
- `GOOGLE_CLOUD_PROJECT`: Required for Cloud Trace correlation

#### Benchmarks Infrastructure (`benchmarks/`)

**A/B Testing Framework**: Compare different configurations to optimize agent performance.

**What Can Be A/B Tested**:
1. **Tool Sets**: Atomic (8 specific paragraph tools) vs Consolidated (2 unified tools)
   - Set `DOCUMENT_MCP_TOOL_SET=atomic` or `DOCUMENT_MCP_TOOL_SET=consolidated`
   - Tests whether unified tools improve task completion vs specific tools
2. **Models**: GPT-5 Mini, Claude Haiku 4.5, Gemini 3 Flash via OpenRouter
   - Compare how different LLMs utilize the same tools
3. **Tool Descriptions**: Different prompt styles for the same tools
   - Test if description format affects tool selection accuracy

**Scenario Levels**:
- Level 1: Simple (single tool selection)
- Level 2: Sequential (2-3 tool chains)
- Level 3: Complex (context-dependent reasoning)
- Level 4: Ambiguous (multiple valid solutions)
- Level 5: Edge cases (empty documents, unicode, boundaries)
- Level 6: Adversarial (confusing terminology, negative indices)

**Key Components**:
- `benchmarks/config.py`: Configuration and model settings
- `benchmarks/scenarios.py`: Test scenarios across all levels
- `benchmarks/tools.py`: Tool set definitions (atomic vs consolidated)
- `benchmarks/runner.py`: Benchmark execution with timeout protection

## Testing Architecture

### 4-Tier Testing Strategy

| Tier | Focus | LLM Calls | Speed | Coverage |
|------|-------|-----------|-------|----------|
| **Unit** | Isolated components with complete mocking | Zero | Fastest | Functions, validation, error conditions, pagination |
| **Integration** | Real MCP server, mocked LLM responses | Zero | Medium | Agent-server communication, tool execution |
| **E2E** | Complete system with real APIs | Real (managed) | Slowest | Full workflows, real AI responses |
| **Evaluation** | Performance benchmarking | Controlled real | Medium | Standardized scenarios, metrics |
| **Metrics** | OpenTelemetry collection validation | Zero | Fast | Production monitoring system |

### Key Testing Requirements

**Integration Tests**:
- Use real MCP stdio transport with mocked LLM responses
- Assert on `details` field content, not LLM-generated `summary`
- Agent implementations must populate `details` field with structured MCP tool responses

**E2E Tests**:
- Use CLI subprocess calls and MCP stdio transport (never import agents/tools directly)
- Assert on file system state and document operations
- Include response validation patterns and API key availability checks

### Testing Infrastructure

**Fixtures System** (`tests/conftest.py`): Test isolation, MCP client management, mock environments, test data factories

**Shared Framework** (`tests/shared/`): Reusable test patterns, mock factories, cleanup management

**Dynamic Tool Descriptions** (`src/agents/shared/tool_descriptions.py`): Centralized tool management with format-specific generation (Full, Compact, Minimal) enabling 5-83% token reduction potential

## Architecture Decisions and Design Principles

### Architecture Overview

- Clear separation of concerns between agents, MCP tool server, and testing
- Four-tiered testing strategy (unit, integration, E2E, evaluation)
- Both agent types have error handling and support multiple LLM providers
- Tool architecture with scope-based operations and pagination

### Key Design Principles

- **Dynamic Tool Descriptions**: Centralized tool management with format-specific generation optimized for each agent architecture
- **Architecture-Specific Optimization**: Each agent uses tool descriptions optimized for its operational pattern (Compact for Simple, Full for ReAct)
- **Token Optimization**: Format selection enables significant token reduction potential while maintaining functionality
- **Maintainability Focus**: Single source of truth for all tools eliminates duplication and simplifies maintenance
- **Clean Naming Conventions**: Consistent "scope-based tools" terminology throughout the codebase for clarity
- **Streamlined Tool Architecture**: Optimized tool structure with enhanced functionality through scope-based operations
- **Universal Safety System**: Automatic snapshot protection with user tracking across all content-modifying tools

### Architecture Decision: Simplified Agent Model

The system uses a **two-agent architecture** optimized for different complexity levels:

```
User Query Complexity → Agent Selection:

Simple Operations → Simple Agent (direct tool calls)
Multi-step Workflows → Simple Agent (sequential operations)
Complex Reasoning → ReAct Agent (reasoning + execution)
```

**Key Insight**: Modern LLMs can handle complex multi-step operations effectively through either direct sequential execution (Simple Agent) or through reasoning workflows (ReAct Agent).

### Semantic Search Integration
- **Tool**: `find_similar_text` with scope-based targeting (`document` or `chapter`)
- **Technology**: Google Gemini embeddings API with cosine similarity matching
- **Features**: Configurable similarity thresholds, result limiting, context snippets
- **Performance**: Batch embedding optimization for efficient API usage
- **Caching**: Snapshot-style embedding cache for 80-90% API call reduction

#### Embedding Cache Architecture
- **Storage Location**: `.embeddings/` directory per document (parallel to `.snapshots/`)
- **Cache Organization**: Chapter-based directory structure mirroring document layout
- **Invalidation Strategy**: Content-based invalidation using file modification times
- **File Format**: Binary numpy arrays (`.npy`) with JSON manifests for metadata
- **Model Versioning**: Separate cache entries for different embedding model versions
- **Performance Benefits**: Sub-second search response times for cached content

### Storage Model Architecture

```
.documents_storage/
├── document_name/           # Document directory
│   ├── 01-chapter.md       # Ordered chapter files (with optional YAML frontmatter)
│   ├── 02-chapter.md
│   ├── summaries/          # Fine-grain summary system
│   │   ├── document.md     # Document-level summary
│   │   ├── 01-chapter.md   # Chapter-specific summary
│   │   └── overview.md     # Section-level summary
│   ├── metadata/           # Structured metadata directory (Phase 2)
│   │   ├── entities.yaml   # Characters, locations, items with aliases
│   │   └── timeline.yaml   # Chronological events with chapter references
│   ├── .snapshots/         # Automatic snapshots directory
│   │   ├── snapshot_20250712_150000_create_chapter_clchinkc/
│   │   └── snapshot_20250712_150030_replace_paragraph_clchinkc/
│   └── .embeddings/        # Embedding cache directory
│       ├── 01-chapter.md/  # Chapter-specific cache
│       │   ├── paragraph_0.npy     # Binary embedding files
│       │   ├── paragraph_1.npy
│       │   └── manifest.json       # Cache metadata
│       └── 02-chapter.md/
│           ├── paragraph_0.npy
│           └── manifest.json
```

### Metadata System Architecture (Phase 2)

**YAML Frontmatter in Chapters:**
```yaml
---
status: draft|revised|complete
pov_character: Marcus
tags: [action, dialogue]
notes: Author notes here
---

# Chapter content here...
```

**Entities Metadata (`metadata/entities.yaml`):**
```yaml
characters:
  - name: Marcus Chen
    aliases: [Marcus, Marc, The Detective]
    type: character
    description: Main protagonist

locations:
  - name: Central Station
    aliases: [the station, HQ]
    type: location
```

**Gemini API Compatibility:**
Tool parameters use individual typed fields instead of `dict[str, Any]` to avoid the `additionalProperties` limitation in Gemini's function calling. This ensures full compatibility across OpenAI, Gemini, and other LLM providers.

### Pagination Architecture

The system implements pagination with the following features:

**Core Implementation:**
- **Memory Efficiency**: `_paginate_document_efficiently()` loads only requested page content with bounded memory usage
- **Character Boundaries**: Proper pagination without breaking content mid-sentence
- **Token Optimization**: 50K characters ≈ 12K tokens per page (configurable)
- **Navigation Metadata**: Full pagination info with total pages, navigation hints, has_more/has_previous flags

**Pydantic Models:**
```python
class PaginationInfo(BaseModel):
    page: int
    page_size: int
    total_characters: int
    total_pages: int
    has_more: bool
    has_previous: bool
    next_page: int | None = None
    previous_page: int | None = None

class PaginatedContent(BaseModel):
    content: str
    document_name: str
    scope: str
    chapter_name: str | None = None
    paragraph_index: int | None = None
    pagination: PaginationInfo
```

**Performance Characteristics:**
- **Before Migration**: Full document loading then truncation (inefficient for large documents)
- **After Migration**: Page-based loading with predictable response sizes
- **Benefits**: Complete data access, no silent truncation, progressive disclosure patterns

### Possible Improvements

- **A/B Testing Framework**: Validation of format optimizations with actual LLM performance metrics
- **Context-Aware Formatting**: Format selection based on query complexity and agent context


## Performance Optimization

### Agent Caching
The ReAct agent implements caching:
```python
# Agent instances cached by model type and prompt hash
_agent_cache = {}  # Avoids repeated initialization
```

### Error Handling Patterns
```python
# Circuit breaker pattern for external services
circuit_breaker = get_circuit_breaker("service_name")
result = await circuit_breaker.call(operation)

# Intelligent retry with exponential backoff
retry_manager = RetryManager()
result = await retry_manager.execute_with_retry(operation)
```

### LLM Call Optimization
- **Unit tests**: Zero LLM calls (fully mocked)
- **Integration tests**: Zero LLM calls (mocked responses)  
- **E2E tests**: Minimized with delays and batching
- **Production**: Circuit breakers prevent cascade failures
- **Sequential Operations**: Streamlined execution of multi-step workflows

## CI/CD and GitHub Actions

**CI Configuration** (`.github/workflows/python-test.yml`):
- Uses `uv` for fast dependency installation, `ruff` for linting, `mypy` for type checking
- Runs unit, integration, and E2E tests with Python 3.13
- Requires API keys for E2E tests, uploads coverage to Codecov

```bash
# Automated test command:
uv run pytest tests/unit/ tests/integration/ tests/e2e/ --timeout=600
```

## Common Workflows

### Adding New Document Tools
1. **Add tool function** to appropriate category module in `document_mcp/tools/`
2. **Define input/output models** with Pydantic validation in `document_mcp/models.py`
3. **Add tool description** to `src/agents/shared/tool_descriptions.py`
4. **Update tool registration** in `document_mcp/tools/__init__.py`
5. **Add unit tests** in `tests/unit/test_doc_tool_server.py`
6. **Add integration tests** for agent interaction
7. **Test with both agent types** (Simple and ReAct)

### Debugging Agent Issues
1. **Check configuration**: `--check-config` flag
2. **Test with simple queries** before complex ones
3. **Review structured logs** in `document_mcp/` directory
4. **Use interactive mode** for step-by-step debugging
5. **Check E2E test patterns** for similar workflows

### Extending Agent Behavior
1. **Agent Prompts**: Modify `get_<agent>_system_prompt()` functions for prompt changes
2. **Tool Descriptions**: Update `src/agents/shared/tool_descriptions.py` for new tools
3. **Format Optimization**: Adjust format types in `get_tool_descriptions_for_agent()`
4. **Add response models** in Pydantic for structured output
5. **Test prompt changes** with unit and integration tests
6. **Validate with E2E tests** for real-world scenarios
7. **Optimize prompts** using `python3 -m prompt_optimizer simple` or `react` for automated improvements

### Optimizing Agent Prompts
The system includes an automated prompt optimizer with comprehensive performance evaluation:

```bash
# Optimize specific agent
python3 -m prompt_optimizer simple
python3 -m prompt_optimizer react  

# Optimize all agents
python3 -m prompt_optimizer all

# Development use within repo only
uv run python -m prompt_optimizer simple
```

**Optimization Features**:
- **Safe Optimization**: Conservative changes that preserve existing functionality
- **Performance-Based**: Uses real execution metrics to evaluate improvements
- **Testing**: Validates changes against all tests
- **Automatic Backup**: Safe rollback if optimization fails
- **Multi-Agent Support**: Works with Simple and ReAct agents

**How It Works**:
1. **Baseline Measurement**: Measures current prompt performance
2. **Conservative Optimization**: LLM generates minimal, safe improvements
3. **Validation**: Runs tests plus performance benchmarks
4. **Decision Logic**: Accepts only if tests pass AND performance improves
5. **Safety First**: Automatic backup and restore if anything breaks

**File Locations**:
- **Agent Prompts**: `src/agents/{agent_type}/prompts.py`
- **Backups**: `prompt_backups/{agent}_prompt_backup_{timestamp}.py`
- **Tests**: `tests/unit/`, `tests/integration/`, `tests/e2e/`

## Troubleshooting

### Common Issues

#### API Authentication
```bash
# Check configuration
python3 src/agents/simple_agent/main.py --check-config

# Set environment variables
export OPENAI_API_KEY="your-key"
# or
export GEMINI_API_KEY="your-key"
```

#### Test Failures
```bash
# Run all tests:
uv run pytest tests/unit/ tests/integration/ --tb=short -q

# Run specific tiers:
uv run pytest tests/unit/ -v
uv run pytest tests/integration/ -v
uv run pytest tests/e2e/ -v --timeout=600
uv run pytest tests/evaluation/ -v
```

#### Performance Issues
- **ReAct Agent**: High latency due to multi-step reasoning
- **Simple Agent**: Faster for single operations
- **E2E Tests**: External API delays (expected)
- **Integration Tests**: Should be fast (check MCP server)

### Debugging Strategies

#### Log Analysis
```bash
# Structured logs in JSON format
tail -f document_mcp/doc_operations.log | jq .

# Error-specific logs
grep "ERROR" document_mcp/errors.log

# MCP call tracking
tail -f document_mcp/mcp_calls.log
```

#### Test Debugging
```python
# Use test fixtures for isolation
def test_my_feature(test_docs_root, sample_document):
    # Isolated test environment provided

# Mock external dependencies
def test_agent_logic(mock_complete_test_environment):
    # All external calls mocked
```

## E2E Testing Best Practices

### Key Insights
- **Response Validation**: Always validate response objects and use defensive programming with null checks
- **API Limitations**: External APIs have quota limits and variable response formats - design tests accordingly
- **Test Focus**: E2E tests validate integration, not production resilience. Keep retry logic in application layer
- **Debugging**: Log response content, monitor MCP server logs, test incrementally

### Failure Categories
- **Code Issues**: Agent logic or MCP integration bugs
- **Infrastructure**: Test setup or environment configuration
- **External**: API quotas, rate limiting, service availability

### Development Guidelines
1. Implement response validation patterns from the start
2. Use centralized timing configuration for API operations  
3. Design tests resilient to external service variability
4. Focus on integration verification, not edge case handling

## Quality Standards

### Code Quality
- **Type hints**: Pydantic models
- **Error handling**: Structured error categories and recovery
- **Logging**: JSON-structured with OpenTelemetry
- **Testing**: 4-tier strategy
- **Documentation**: Docstrings and inline comments

### Performance Standards
- **Unit tests**: < 1s per test file
- **Integration tests**: < 10s per test file
- **E2E tests**: < 60s per test (with API delays)
- **Tool operations**: < 100ms for document operations
- **Agent responses**: < 30s for complex workflows

### Reliability Standards
- **Error recovery**: Circuit breakers and retry logic implemented
- **Test stability**: Unit and integration tests passing
- **E2E reliability**: Full workflows tested with real APIs

## Test Status Summary

| Test Tier | Description |
|-----------|-------------|
| Unit | Isolated components, pagination, observability |
| Integration | Agent-MCP communication, tool execution |
| E2E | Full system workflows with real APIs |
| Evaluation | Performance benchmarking |
| Metrics | OpenTelemetry collection |

Run `uv run pytest` to verify all tests pass.


## Development Best Practices

### Testing Strategy
**Test Hierarchy**: Unit (mocked) → Integration (real MCP, mocked LLM) → E2E (real APIs) → Evaluation (performance benchmarks)

**Key Requirements**:
- Assert on `details` field content, not LLM-generated `summary` text
- E2E tests use CLI subprocess calls and MCP stdio transport
- Agent implementations MUST populate `details` field with structured MCP tool responses

### Testing Patterns
```python
# Async testing
@pytest.mark.asyncio
async def test_async_function():
    result = await some_async_function()
    assert result == expected

# MCP integration testing  
async def test_mcp_integration(mcp_client):
    response = await mcp_client.call_tool("list_documents", {})
    assert response["documents"] == []
```

### Common Issues & Solutions
- **Import Errors**: Ensure project root in PYTHONPATH, check circular imports
- **Fixture Issues**: Verify fixture scope and conftest.py location
- **E2E Failures**: Check API keys, rate limits, network connectivity
- **Async Issues**: Use `@pytest.mark.asyncio` decorator, proper await usage

### Development Guidelines
**Agent Development**: Start with Simple Agent for prototyping, use ReAct for production. Test prompt changes thoroughly.

**Tool Development**: Follow atomic operation principles, implement validation, add structured logging, design for idempotency.


## Codebase Structure

*   **`src/agents`**: This directory contains the core agent logic with a clean, modular structure.
    *   `simple_agent/`: A package-based agent implementation with organized modules:
        *   `main.py`: Core agent execution logic
        *   `prompts.py`: System prompt definitions
    *   `react_agent/`: A more complex agent that follows the ReAct (Reason + Act) paradigm. It has its own sub-modules for `main`, `models`, `parser`, and `prompts`.
    *   `shared/`: Contains code shared between the different agents:
        *   `cli.py`: Common command-line interface functionality
        *   `config.py`: Enhanced Pydantic Settings for configuration management
        *   `error_handling.py`: Shared error handling utilities
*   **`tests/`**: This directory is well-organized into different types of tests.
    *   `e2e/`: End-to-end tests, which test the entire system. `test_agents_e2e.py` runs tests against the agents with real APIs.
    *   `integration/`: Integration tests. `test_agents_stdio.py` tests the agents' input/output, and `test_doc_tool_server.py` tests the document tool server.
    *   `unit/`: Unit tests, which test individual components. There are tests for `atomic_paragraph_tools`, `doc_tool_server`, and the `react_agent_parser`.